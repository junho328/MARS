# Multi-Agent LoRA Adapter Training for Hanabi
# Each player gets a dedicated LoRA adapter that is trained independently.
#
# Usage:
#   python examples/start_agentic_pipeline.py --config examples/hanabi/multi_agent_hanabi.yaml
#
# Key features:
# - multi_adapter_mode: true enables per-agent LoRA adapters
# - num_agents: 2 (one adapter per Hanabi player)
# - Each adapter only receives gradients from its own agent's trajectories

defaults:
  - ../config/envs@_here_
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

exp_name: "multi_agent_hanabi"
seed: 42
logging_dir: ./output/${exp_name}/logs
output_dir: ./output/${exp_name}

system_envs:
  USE_MODELSCOPE: "1"

track_with: wandb
tracker_kwargs:
  project: mars-hanabi-multi-agent
  name: hanabi_multi_agent_${exp_name}
  notes: "Hanabi multi-agent training with per-agent LoRA adapters"
  dir: ./output

# LoRA configuration
lora_target: q_proj,k_proj,v_proj,o_proj
lora_rank: 32
lora_alpha: 32

# Multi-agent settings
multi_adapter_mode: true
num_agents: 2

num_gpus_per_node: 2

max_steps: 200
save_steps: 20
logging_steps: 1
eval_steps: 5
resume_from_checkpoint: false

rollout_batch_size: 128
val_batch_size: 1500
sequence_length: 32768

# PPO settings
dual_clip_loss: true
ppo_epochs: 1
adv_estimator: "reinforce"
init_kl_coef: 0.0
use_kl_loss: true
kl_loss_coef: 0.20

whiten_advantages: true
entropy_loss_coef: 0

pretrain: Qwen/Qwen3-4B

actor_train:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    # Multi-adapter LoRA settings
    multi_adapter_mode: ${multi_adapter_mode}
    num_agents: ${num_agents}
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
    freeze_base_model: true
    model_type: ~
  training_args:
    lr_scheduler_type: cosine_with_min_lr
    learning_rate: 1e-6
    weight_decay: 0.05
    adam_beta1: 0.9
    adam_beta2: 0.95
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 2
    warmup_steps: 10
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: deepspeed_train
    strategy_config: ${deepspeed_zero3}
  device_mapping: list(range(0,2))
  infer_batch_size: 2

actor_infer:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
  generating_args:
    max_new_tokens: 4096
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.6
    num_return_sequences: 1
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.5
      block_size: 16
      load_format: auto
  device_mapping: list(range(0,2))

reference:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: hf_infer
    strategy_config: ~
  device_mapping: list(range(0,2))
  infer_batch_size: 2

enable_response_mask: True
action_sep: "||"
use_turn_scores: True
enable_think: True
max_actions_per_traj: 50

reward_normalization:
  grouping: tags
  method: mean
  separate_norm_for_selfplay: true

whiten_rewards: true
advantage_norm: mean
pg_clip_high: 0.20

# Hanabi environments for self-play (no built-in opponent)
custom_envs:
  MiniHanabi-SelfPlay:
    env_type: hanabi
    max_actions_per_traj: ${max_actions_per_traj}
    grid_vocab: false
    env_config:
      render_mode: text
      built_in_opponent: none  # Self-play mode
      players: 2

  SimpleHanabi-SelfPlay:
    env_type: hanabi
    max_actions_per_traj: ${max_actions_per_traj}
    grid_vocab: false
    env_config:
      render_mode: text
      built_in_opponent: none
      players: 2
      colors: 3
      ranks: 2
      hand_size: 5
      max_information_tokens: 8
      max_life_tokens: 3

  FullHanabi-SelfPlay:
    env_type: hanabi
    max_actions_per_traj: ${max_actions_per_traj}
    grid_vocab: false
    env_config:
      render_mode: text
      built_in_opponent: none
      players: 2
      colors: 3
      ranks: 3
      hand_size: 5
      max_information_tokens: 8
      max_life_tokens: 3

train_env_manager:
  format_penalty: 0.05
  max_env_num_per_worker: 64
  env_groups: 64
  group_size: 1
  tags: [MiniHanabi-SelfPlay]
  n_groups: [64]

val_env_manager:
  max_env_num_per_worker: 32
  env_groups: 96
  group_size: 1
  tags: [
    MiniHanabi-SelfPlay,
    SimpleHanabi-SelfPlay,
    FullHanabi-SelfPlay
  ]
  n_groups: [32, 32, 32]

