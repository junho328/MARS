# bash examples/bridge/run_agentic_pipeline_bridge_selfplay.sh
# tensorboard --logdir=/ext_hdd/jhna/mars/exps/agentic_pipeline/output/tensorboard_logs

defaults:
  - ../config/envs@_here_
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

exp_name: bridge_selfplay
seed: 42
logging_dir: /ext_hdd/nhkoh/MARS/output/mars/exps/${exp_name}/output/logs
output_dir: /ext_hdd/nhkoh/MARS/output/mars/exps/${exp_name}/output

# render_save_dir: ./runs/render
system_envs:
  USE_MODELSCOPE: "1"

track_with: ~
tracker_kwargs: ~

# track_with: tensorboard
# tracker_kwargs:
#   log_dir: /ext_hdd/nhkoh/MARS/output/mars/exps/${exp_name}/output/tensorboard_logs

# track_with: wandb
# tracker_kwargs:
#  api_key: 4f27a301fcc24e0267973277fc29e90b1c78064b
#  project: mars-bridge-train
#  name: bridge_train_${exp_name}
#  notes: "Bridge self-play train with random opponent"
#  dir: /ext_hdd/nhkoh/MARS/output/mars/exps/${exp_name}/output/logs

# track_with: tensorboard
# tracker_kwargs:
#   log_dir: /ext_hdd/jhna/mars/exps/${exp_name}/output/tensorboard_logs

lora_target: o_proj,q_proj,k_proj,v_proj
lora_rank: 32
lora_alpha: 32
lora_dropout: 0.0
lora_bias: "none"

num_gpus_per_node: 1

max_steps: 50
save_steps: 10
logging_steps: 1
eval_steps: 5
resume_from_checkpoint: false

rollout_batch_size: 64
val_batch_size: 500
sequence_length: 8192

# reward_clip: 10
# advantage_clip: 4
dual_clip_loss: true
ppo_epochs: 1
adv_estimator: "reinforce"
# pg_clip: 0.1
#dual_clip_loss: True
init_kl_coef: 0.0
use_kl_loss: true
kl_loss_coef: 0.20

whiten_advantages: true
entropy_loss_coef: 0

pretrain: Qwen/Qwen3-1.7B

actor_train:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
    lora_dropout: ${lora_dropout}
    lora_bias: ${lora_bias}
    model_type: ~
  training_args:
    lr_scheduler_type: cosine_with_min_lr
    learning_rate: 1e-6
    weight_decay: 0.05
    adam_beta1: 0.9
    adam_beta2: 0.95
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 4
    warmup_steps: 10
  data_args:
    template: qwen3
  # strategy_args:
  #   strategy_name: megatron_train
  #   strategy_config:
  #     sequence_parallel: true
  #     tensor_model_parallel_size: 2
  #     pipeline_model_parallel_size: 1
  #     expert_model_parallel_size: 1
  #     use_distributed_optimizer: true
  #     recompute_granularity: full
  #     accumulate_allreduce_grads_in_fp32: true
  #     check_for_nan_in_loss_and_grad: false
  strategy_args:
    strategy_name: deepspeed_train
    strategy_config: ${deepspeed_zero3}
  device_mapping: list(range(0,1))
  infer_batch_size: 2

actor_infer:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    lora_target: ${lora_target}
    lora_rank: ${lora_rank}
    lora_alpha: ${lora_alpha}
  generating_args:
    max_new_tokens: 4096 # single-turn response length
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.6
    num_return_sequences: 1
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.8
      block_size: 16
      load_format: auto
  device_mapping: list(range(0,1))

reference:
  model_args:
    flash_attn: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: hf_infer
    strategy_config: ~
  device_mapping: list(range(0,1))
  infer_batch_size: 2

enable_response_mask: True
action_sep: "||"
use_turn_scores: True # important to GAE when applying token-level rewards to token-level advantages. If False, will take the sum of scores as the reward for the last turn.
enable_think: True # False -> no think RL
max_actions_per_traj: 100
reward_normalization:
  grouping: tags # 可以tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by计算reward/adv
  method: mean # asym_clip / identity / mean_std
  separate_norm_for_selfplay: true
whiten_rewards: true
advantage_norm: mean
# loss_agg_mode: token-mean
pg_clip_high: 0.20

custom_envs:
  # Contract Bridge - 정식 4인 브릿지 (비딩 + 플레이)
  ContractBridge:
    env_type: bridge
    max_actions_per_traj: ${max_actions_per_traj}
    grid_vocab: false
    env_config:
      built_in_opponent: random
      render_mode: text
      dealer: 0
      dealer_vul: false
      non_dealer_vul: false

train_env_manager:
  format_penalty: 0.05 # penalty for wrong format
  max_env_num_per_worker: 32
  env_groups: 32
  # under the same group, the env config and env seed are ensured to be equal
  group_size: 1
  tags: [ContractBridge]
  n_groups: [32] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

val_env_manager:
  max_env_num_per_worker: 16
  env_groups: 16
  group_size: 1
  tags: [
    ContractBridge
  ]
  n_groups: [16] #
